<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->


  <title>GAMap</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation</h1>
            <!-- <div class="is-size-5 publication-authors"> -->
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a>Shuaihang Yuan</a>,</span>
                <span class="author-block">
                  <a>Hao Huang</a>,</span>
                  <span class="author-block">
                    <a>Yu Hao</a>,</span>
                    <span class="author-block">
                      <a>Congcong Wen</a>,</span>
                      <span class="author-block">
                        <a>Anthony Tzes</a>,</span>
                        <span class="author-block">
                          <a>Yi Fang</a>
                  </span>
                  </div> -->

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">NYUAD Center for Artificial Intelligence and Robotics (CAIR)<br>
                      New York University Abu Dhabi, Electrical Engineering<br>
                      New York University, Electrical & Computer Engineering Dept<br>
                      Embodied AI and Robotics (AIR) Lab<br>NeurIPS 2024</span>
                  </div> -->

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://nips.cc/virtual/2024/poster/95755" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a> -->
                    <!-- </span> -->


                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/shalexyuan/GAMap/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.23978" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            <!-- </div> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Replacing video with a pipeline figure image -->
      <img src="static/images/pipeline.png" alt="Pipeline Figure" width="100%">
      <h2 class="subtitle has-text-centered">
        Pipeline of the proposed method.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Instance Image-Goal Navigation (IIN) requires autonomous agents to identify and navigate to a target object or location depicted in a reference image captured from any viewpoint. While recent methods leverage powerful novel view synthesis (NVS) techniques, such as three-dimensional Gaussian splatting (3DGS), they typically rely on randomly sampling multiple viewpoints or trajectories to ensure comprehensive coverage of discriminative visual cues. This approach, however, creates significant redundancy through overlapping image samples and lacks principled view selection, substantially increasing both rendering and comparison overhead. In this paper, we introduce a novel IIN framework with a hierarchical scoring paradigm that estimates optimal viewpoints for target matching. Our approach integrates cross-level semantic scoring, utilizing CLIP-derived relevancy fields to identify regions with high semantic similarity to the target object class, with fine-grained local geometric scoring that performs precise pose estimation within promising regions. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on simulated IIN benchmarks and real-world applicability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Results Figure Presentation. -->
      <h2 class="title is-3">Simulated Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="figure-display">
            <!-- Single Large Figure -->
            <img src="static/images/navigation.png" alt="Result Figure 1" style="width:100%;">
            <h2 class="subtitle has-text-centered">
              Navigation trajectories
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
    <!-- YouTube Video Presentation -->

</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Results Figure Presentation. -->
      <h2 class="title is-3">Hiericarial Scoring Visualization</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="figure-display">
            <!-- Single Large Figure -->
            <img src="static/images/extra_gaussian.png" alt="Hiericarial Scoring" style="width:100%;">
            <h2 class="subtitle has-text-centered">
              Hiericarial Scoring.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
    <!-- YouTube Video Presentation -->

</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Real-World Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/QYLM2XtZCK8?si=XVZckXBvLuLqq9bR" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
